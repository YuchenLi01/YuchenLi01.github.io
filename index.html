<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuchen Li</title>

  <meta name="author" content="me">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/cmu_scs_dragon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:3%">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:0%;width:65%;max-width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuchen Li</name>
              </p>
              <p>I am a Ph.D. student in the <a href="https://www.ml.cmu.edu/">Machine&nbsp;Learning Department</a> at <a href="https://www.cmu.edu/">Carnegie&nbsp;Mellon University</a>, where
                 I am fortunate to be advised by Professor <a href="https://www.andrew.cmu.edu/user/aristesk/">Andrej&nbsp;Risteski</a>.
              </p>
              <p>
                Previously, I spent four wonderful years at the <a href="https://illinois.edu/">University of Illinois at Urbana-Champaign</a>,
                completing my bachelor's degree in Statistics & Computer Science, as well as Mathematics.
                I am grateful for the mentorship of Professor <a href="http://hanj.cs.illinois.edu/">Jiawei&nbsp;Han</a>, Professor <a href="https://faculty.math.illinois.edu/~hildebr/">AJ&nbsp;Hildebrand</a>, and Professor <a href="http://pramodv.ece.illinois.edu/">Pramod&nbsp;Viswanath</a>.
              </p>
              <p>
                In the industry, I spent some exciting time at <a href="https://www.quora.com/about">Quora</a>
                and <a href="https://about.facebook.com/">Facebook</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:yuchenl4@cs.cmu.edu">Email</a> &nbsp/&nbsp
                <a href="data/Yuchen-Li-cv.pdf">CV</a> &nbsp/&nbsp
<!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp-->
                <a href="https://www.linkedin.com/in/yuchenli01/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=2Uu9K30AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/_Yuchen_Li_">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/YuchenLi01/">GitHub</a>
              </p>
            </td>
            <td style="padding:0%;width:35%;max-width:35%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/adv28.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <br><br>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0%;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I study machine learning, natural language processing, and data mining.
                My long-term goals include improving mathematical understanding of empirical phenomena, and developing principled approaches to modern deep learning applications.
              </p>
            </td>
          </tr>
        </tbody></table>
        <br><br>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0%;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                In the following list, the asterisk symbol (*) in the author list means equal contribution or alphabetical order.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td style="padding:0%;width:35%;vertical-align:middle">
              <img src="images/aistats_logo.jpeg" alt="aistats_logo" style="width:100%;max-width:100%">
            </td>
            <td width="75%" valign="middle">
<!--              <a href="">-->
                <papertitle>Contrasting the landscape of contrastive and non-contrastive learning</papertitle>
<!--              </a>-->
<!--              <br>-->
<!--                <strong>Yuchen Li*</strong>,-->
<!--                Andrej Risteski*-->
<!--              <br><br>-->
              <em>Conference on Artificial Intelligence and Statistics (AISTATS)</em>, 2022
              <br>
<!--              <a href="data/li2021limitations.bib">bibtex</a> /-->
<!--              <a href="data/li2021limitations_slides.pdf">slides</a> /-->
<!--              <a href="https://twitter.com/_yuchen_li_/status/1402413507115692035?s=21">My Twitter post</a> /-->
<!--              <a href="https://twitter.com/risteski_a/status/1402110089801179139?s=21">Andrej's Twitter post</a>-->
              <p>Recently accepted. More details to come!
              </p>
              <p>We show through theoretical results and controlled experiments that even on simple data models,
                non-contrastive losses have a preponderance of <em>non-collapsed bad minima</em>
                (which do not exist for their contrastive loss counterpart).
                Moreover, we show that the training process does not avoid these minima.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:0%;width:35%;vertical-align:middle">
              <img src="images/pcfg_context.png" alt="pcfg_context" style="width:100%;max-width:100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2106.01580">
                <papertitle>The Limitations of Limited Context for Constituency Parsing</papertitle>
              </a>
              <br>
                <strong>Yuchen Li*</strong>,
                Andrej Risteski*
              <br><br>
              <em>Association for Computational Linguistics (ACL)</em>, 2021
              <br>
              <a href="data/li2021limitations.bib">bibtex</a> /
              <a href="data/li2021limitations_slides.pdf">slides</a> /
              <a href="https://twitter.com/_yuchen_li_/status/1402413507115692035?s=21">My Twitter post</a> /
              <a href="https://twitter.com/risteski_a/status/1402110089801179139?s=21">Andrej's Twitter post</a>
              <p>We prove that, if the context for the parsing decision at each word is <em>unbounded</em> & <em>bidirectional</em>,
                then the parsing model has full representational power.
              </p>
              <p>On the other hand, even if the context is bounded in one direction (which is the case for various leading approaches to constituency parsing),
                the parsing power is quite limited for hard grammars.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:0%;width:35%;vertical-align:middle">
              <img src="images/complexity.png" alt="complexity" style="width:100%;max-width:100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1804.00221">
                <papertitle>Complexity of Leading Digit Sequences</papertitle>
              </a>
              <br>
                Xinwei He*,
                AJ Hildebrand*,
                <strong>Yuchen Li*</strong>,
                Yunyi Zhang*
              <br><br>
              <em>Journal of Discrete Mathematics & Theoretical Computer Science (DMTCS)</em>, vol. 22 no. 1, Automata, Logic and Semantics, 2020
              <br>
              <a href="data/he2020complexity.bib">bibtex</a> /
              <a href="data/Complexity_Poster_JMM_2018.pdf">poster</a>
              <p>Consider the sequence of leading digits of 2^n: 1, 2, 4, 8, 1, 3, 6, ......
                What is the <em>complexity</em> of such sequences?
                They are neither <em>periodic</em> nor <em>random</em>.
                How to quantify and distinguish the middle ground?
              </p>
              <p>We prove that, the <em>block complexity is linear</em> for the sequence of leading digits of a^n in base b (except for some special cases).
                In fact, we give explicit formula for the linear coefficients in terms of a and b.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:0%;width:35%;vertical-align:middle">
              <img src="images/HyperMine.png" alt="HyperMine" style="width:100%;max-width:100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1909.01584">
                <papertitle>Discovering Hypernymy in Text-Rich Heterogeneous Information Network by Exploiting Context Granularity</papertitle>
              </a>
              <br>
                Yu Shi*,
                Jiaming Shen*,
                <strong>Yuchen Li</strong>,
                Naijing Zhang,
                Xinwei He,
                Zhengzhi Lou,
                Qi Zhu,
                Matthew Walker,
                Myunghwan Kim,
                Jiawei Han
              <br><br>
              <em>Conference on Information and Knowledge Management (CIKM)</em>, 2019
              <br>
              <a href="data/shi2019discovering.bib">bibtex</a>
              <p>This work discovers hypernym-hyponym ("is-a" relation) pairs by an innovative combination of information from natural language and graph structure.
                Our system outperforms existing methods that utilize signals from text, graph, or both.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:0%;width:35%;vertical-align:middle">
              <img src="images/spell_correction.png" alt="spell_correction" style="width:100%;max-width:100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1901.07688">
                <papertitle>Context-Sensitive Malicious Spelling Error Correction</papertitle>
              </a>
              <br>
                Hongyu Gong,
                <strong>Yuchen Li</strong>,
                Suma Bhat,
                Pramod Viswanath
              <br><br>
              <em>The Web Conference (WWW)</em>, 2019
              <br>
              <a href="data/gong2019context.bib">bibtex</a>
              <p>Given a misspelled word, our algorithm generates candidates based on the surface form,
                represents the context as a linear space, and
                selects the candidate whose embedding is closest to the context subspace.
                The spelling correction accuracy is better than the state-of-the-art methods, improving the performance in downstream applications.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:0%;width:35%;vertical-align:middle">
              <img src="images/hin_motif.png" alt="hin_motif" style="width:100%;max-width:100%">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.mlgworkshop.org/2018/papers/MLG2018_paper_46.pdf">
                <papertitle>Temporal Motifs in Heterogeneous Information Networks</papertitle>
              </a>
              <br>
                <strong>Yuchen Li*</strong>,
                Zhengzhi Lou*,
                Yu Shi,
                Jiawei Han
              <br><br>
              <em>Workshop on Mining and Learning with Graphs (MLG), in conjunction with KDD</em>, 2018
              <br>
              <a href="data/li2018temporal.bib">bibtex</a>
              <p>This work shows the effectiveness of temporal motifs (frequent substructures)
                in quantifying the relationship between nodes in heterogeneous information networks,
                and proposes efficient algorithms for counting a family of useful motifs.
              </p>
            </td>
          </tr>

        </tbody></table>
        <br><br>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0%;width:100%;vertical-align:middle">
              <heading>Talks</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:0%;width:35%;vertical-align:middle">
              <img src="images/pcfg_context.png" alt="pcfg_context" style="width:100%;max-width:100%">
            </td>
            <td width="75%" valign="center">
              <papertitle>The Limitations of Limited Context for Constituency Parsing</papertitle>
              <br>
              <a href="data/li2021limitations_slides.pdf">slides</a>
              <br><br>
              2021-08
              <a href="https://2021.aclweb.org/">Association for Computational Linguistics (ACL) Conference</a>
              <br><br>
              2021-07
              <a href="https://www.neclab.eu/">NEC Laboratories Europe</a>
              <br><br>
              2021-06
              <a href="https://acmilab.org/">Approximately Correct Machine Intelligence (ACMI) Lab, CMU</a>
            </td>
          </tr>
        </tbody></table>
        <br><br>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0%">
              <br>
              <p style="text-align:right;font-size:small;">
                Template of this page: credit to: <a href="https://github.com/jonbarron/jonbarron_website">link</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <br><br>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;display:none;"><tbody>
          <tr style="display:none;">
            <td style="display:none;">
              <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=MtOmJ7736hKXGMe1JbPtYsUl79r7Tt6Lk3vJygxnrHw&cl=ffffff&w=a"></script>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </tbody></table>
</body>

</html>
